{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudscraper in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.2.71)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cloudscraper) (3.0.9)\n",
      "Requirement already satisfied: requests>=2.9.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cloudscraper) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install cloudscraper\n",
    "import cloudscraper\n",
    "scraper = cloudscraper.create_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cfscrape in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: requests>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cfscrape) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.6.1->cfscrape) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.6.1->cfscrape) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.6.1->cfscrape) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.6.1->cfscrape) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install cfscrape\n",
    "import cfscrape\n",
    "scraper = cfscrape.create_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://hansard.parliament.uk/commons/2023-10-26\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# List of user agents to rotate through\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "# Base URL for the Hansard site\n",
    "base_url = 'https://hansard.parliament.uk'\n",
    "\n",
    "# Function to build the full URL for a given date\n",
    "def build_url_for_date(date):\n",
    "    return f\"{base_url}/commons/{date}\"\n",
    "\n",
    "# Function to get all debate URLs for a given date that include 'OralAnswersToQuestions' in their href\n",
    "def get_oral_answers_urls_for_date(date):\n",
    "    url = build_url_for_date(date)\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"Connection error occurred: {conn_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"Timeout error occurred: {timeout_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"An error occurred during the request: {req_err}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = soup.find_all('a', class_='card card-section', href=lambda h: h and 'OralAnswersToQuestions' in h)\n",
    "    full_urls = [base_url + link['href'] for link in links]\n",
    "\n",
    "    return full_urls\n",
    "\n",
    "# Example usage\n",
    "date = '2023-10-26'\n",
    "oral_answers_urls = get_oral_answers_urls_for_date(date)\n",
    "\n",
    "for url in oral_answers_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://hansard.parliament.uk/commons/2023-10-26\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of Safari user agents to rotate through\n",
    "safari_user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "# Base URL for the Hansard site\n",
    "base_url = 'https://hansard.parliament.uk'\n",
    "\n",
    "# Function to build the full URL for a given date\n",
    "def build_url_for_date(date):\n",
    "    return f\"{base_url}/commons/{date}\"\n",
    "\n",
    "# Function to get all debate URLs for a given date that include 'OralAnswersToQuestions' in their href\n",
    "def get_oral_answers_urls_for_date(date):\n",
    "    url = build_url_for_date(date)\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(safari_user_agents),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"Connection error occurred: {conn_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"Timeout error occurred: {timeout_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"An error occurred during the request: {req_err}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = soup.find_all('a', class_='card card-section', href=lambda h: h and 'OralAnswersToQuestions' in h)\n",
    "    full_urls = [base_url + link['href'] for link in links]\n",
    "\n",
    "    return full_urls\n",
    "\n",
    "# Example usage\n",
    "date = '2023-10-26'\n",
    "oral_answers_urls = get_oral_answers_urls_for_date(date)\n",
    "\n",
    "for url in oral_answers_urls:\n",
    "    print(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
