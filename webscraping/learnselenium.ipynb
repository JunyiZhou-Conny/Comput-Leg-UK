{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudscraper in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.2.71)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cloudscraper) (3.0.9)\n",
      "Requirement already satisfied: requests>=2.9.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cloudscraper) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install cloudscraper\n",
    "import cloudscraper\n",
    "scraper = cloudscraper.create_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cfscrape in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: requests>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cfscrape) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.6.1->cfscrape) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.6.1->cfscrape) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.6.1->cfscrape) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.6.1->cfscrape) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install cfscrape\n",
    "import cfscrape\n",
    "scraper = cfscrape.create_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://hansard.parliament.uk/commons/2023-10-26\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# List of user agents to rotate through\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "# Base URL for the Hansard site\n",
    "base_url = 'https://hansard.parliament.uk'\n",
    "\n",
    "# Function to build the full URL for a given date\n",
    "def build_url_for_date(date):\n",
    "    return f\"{base_url}/commons/{date}\"\n",
    "\n",
    "# Function to get all debate URLs for a given date that include 'OralAnswersToQuestions' in their href\n",
    "def get_oral_answers_urls_for_date(date):\n",
    "    url = build_url_for_date(date)\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"Connection error occurred: {conn_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"Timeout error occurred: {timeout_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"An error occurred during the request: {req_err}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = soup.find_all('a', class_='card card-section', href=lambda h: h and 'OralAnswersToQuestions' in h)\n",
    "    full_urls = [base_url + link['href'] for link in links]\n",
    "\n",
    "    return full_urls\n",
    "\n",
    "# Example usage\n",
    "date = '2023-10-26'\n",
    "oral_answers_urls = get_oral_answers_urls_for_date(date)\n",
    "\n",
    "for url in oral_answers_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests-html) (2.31.0)\n",
      "Collecting pyquery (from requests-html)\n",
      "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting fake-useragent (from requests-html)\n",
      "  Downloading fake_useragent-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting parse (from requests-html)\n",
      "  Downloading parse-1.20.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting bs4 (from requests-html)\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting w3lib (from requests-html)\n",
      "  Downloading w3lib-2.1.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: certifi>=2021 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyppeteer>=0.0.14->requests-html) (2023.7.22)\n",
      "Collecting importlib-metadata>=1.4 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading importlib_metadata-7.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyppeteer>=0.0.14->requests-html) (4.66.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.15)\n",
      "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading websockets-10.4-cp311-cp311-macosx_11_0_arm64.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bs4->requests-html) (4.11.1)\n",
      "Requirement already satisfied: lxml>=2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyquery->requests-html) (4.9.3)\n",
      "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->requests-html) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->requests-html) (3.4)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4->bs4->requests-html) (2.3.2.post1)\n",
      "Downloading fake_useragent-1.4.0-py3-none-any.whl (15 kB)\n",
      "Downloading parse-1.20.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=41417a1f52a1fbee9ef1afcbbbe0cb934bc9e84788f4b8b34a5382a4e14e9a69\n",
      "  Stored in directory: /Users/conny/Library/Caches/pip/wheels/d4/c8/5b/b5be9c20e5e4503d04a6eac8a3cd5c2393505c29f02bea0960\n",
      "Successfully built bs4\n",
      "Installing collected packages: pyee, parse, fake-useragent, appdirs, zipp, websockets, w3lib, cssselect, pyquery, importlib-metadata, bs4, pyppeteer, requests-html\n",
      "Successfully installed appdirs-1.4.4 bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.4.0 importlib-metadata-7.0.0 parse-1.20.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 w3lib-2.1.2 websockets-10.4 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "import requests_html\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "\n",
    "\n",
    "s = HTMLSession()\n",
    "url = 'https://hansard.parliament.uk/commons/2023-12-15'\n",
    "\n",
    "r = s.get(url)\n",
    "print(r.status_code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://hansard.parliament.uk/commons/2023-10-26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of Safari user agents to rotate through\n",
    "safari_user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "# Base URL for the Hansard site\n",
    "base_url = 'https://hansard.parliament.uk'\n",
    "\n",
    "# Function to build the full URL for a given date\n",
    "def build_url_for_date(date):\n",
    "    return f\"{base_url}/commons/{date}\"\n",
    "\n",
    "# Function to get all debate URLs for a given date that include 'OralAnswersToQuestions' in their href\n",
    "def get_oral_answers_urls_for_date(date):\n",
    "    url = build_url_for_date(date)\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(safari_user_agents),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"Connection error occurred: {conn_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"Timeout error occurred: {timeout_err}\")\n",
    "        return []\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"An error occurred during the request: {req_err}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = soup.find_all('a', class_='card card-section', href=lambda h: h and 'OralAnswersToQuestions' in h)\n",
    "    full_urls = [base_url + link['href'] for link in links]\n",
    "\n",
    "    return full_urls\n",
    "\n",
    "# Example usage\n",
    "date = '2023-10-26'\n",
    "oral_answers_urls = get_oral_answers_urls_for_date(date)\n",
    "\n",
    "for url in oral_answers_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from webdriver-manager) (2.31.0)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from webdriver-manager) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (2023.7.22)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.0 webdriver-manager-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.15.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selenium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChrome()\n\u001b[1;32m      8\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttp://the-internet.herokuapp.com/login\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m driver\u001b[39m.\u001b[39;49mfind_element_by_xpath(\u001b[39m'\u001b[39m\u001b[39m//*[@id=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39musername\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msend_keys(\u001b[39m'\u001b[39m\u001b[39mtomsmith\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m driver\u001b[39m.\u001b[39mfind_element_by_xpath(\u001b[39m'\u001b[39m\u001b[39m//*[@id=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpassword\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msend_keys(\u001b[39m'\u001b[39m\u001b[39mSuperSecretPassword!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m driver\u001b[39m.\u001b[39mfind_element_by_xpath(\u001b[39m'\u001b[39m\u001b[39m//*[@id=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]/button\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mclick()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_xpath'"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#linux default path /usr/local/bin\n",
    "driver = webdriver.Chrome('/Users/conny/Desktop/chromedriver-mac-arm64/chromedriver')\n",
    "\n",
    "url = 'http://the-internet.herokuapp.com/login'\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"username\"]').send_keys('tomsmith')\n",
    "driver.find_element_by_xpath('//*[@id=\"password\"]').send_keys('SuperSecretPassword!')\n",
    "driver.find_element_by_xpath('//*[@id=\"login\"]/button').click()\n",
    "\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title verification successful!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "chrome_binary = 'binaries/Google Chrome for Testing 120.app/Contents/MacOS/Google Chrome for Testing'\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = chrome_binary\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),options=options)\n",
    "\n",
    "\n",
    "def verify_title():\n",
    "    # chrome_binary_path = \"binaries/Google Chrome for Testing 116.app/Contents/MacOS/Google Chrome for Testing\"\n",
    "\n",
    "    # options = webdriver.ChromeOptions()\n",
    "    # options.binary_location = chrome_binary_path\n",
    "\n",
    "    # # Automatically download and use the latest ChromeDriver\n",
    "    # driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager(driver_version='116').install()), options=options)\n",
    "\n",
    "    # Navigate to the website\n",
    "    driver.get(\"https://sdetunicorns.com\")\n",
    "\n",
    "    # Get the title of the page\n",
    "    title = driver.title\n",
    "\n",
    "    # Verify the title\n",
    "    expected_title = \"Master Software Testing and Automation | SDET Unicorns\"\n",
    "    if title == expected_title:\n",
    "        print(\"Title verification successful!\")\n",
    "    else:\n",
    "        print(f\"Title verification failed. Expected '{expected_title}', but got '{title}'.\")\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "url = 'http://the-internet.herokuapp.com/login'\n",
    "driver.get(url)\n",
    "\n",
    "# Using the new find_element method with By.XPATH\n",
    "driver.find_element(By.XPATH, '//*[@id=\"username\"]').send_keys('tomsmith')\n",
    "driver.find_element(By.XPATH, '//*[@id=\"password\"]').send_keys('SuperSecretPassword!')\n",
    "driver.find_element(By.XPATH, '//*[@id=\"login\"]/button').click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
